# ⚡ PMU 电力故障智能分类系统

## PMU Edge AI Fault Classifier — Protocol v1.2

基于 PMU（相量测量单元）时序数据，使用深度学习模型自动识别电力系统故障类型，支持 TCN、BiLSTM、Transformer 三种模型架构，配备 Streamlit 可视化推理界面。

---

## 目录

1. [项目结构](#1-项目结构)
2. [环境配置](#2-环境配置)
3. [数据说明](#3-数据说明)
4. [特征工程原理](#4-特征工程原理)
5. [模型架构原理](#5-模型架构原理)
6. [训练流程](#6-训练流程)
7. [推理流程](#7-推理流程)
8. [GUI 使用说明](#8-gui-使用说明)
9. [配置文件详解](#9-配置文件详解)
10. [注意事项与常见问题](#10-注意事项与常见问题)
11. [故障类型参考](#11-故障类型参考)

---

## 1. 项目结构

```text
Thesis/
├── Code/
│   ├── app.py                        # GUI 启动入口
│   ├── train.py                      # 命令行训练脚本
│   ├── DataFormatter.ipynb           # 原始数据预处理笔记本（已完成，勿改动）
│   ├── configs/
│   │   └── train_config.yaml         # 训练与推理配置文件
│   ├── models/
│   │   ├── __init__.py
│   │   ├── classifier.py             # TCN / BiLSTM / Transformer 模型定义
│   │   └── feature_engineering.py   # 特征工程、归一化、滑动窗口
│   ├── GUI/
│   │   └── dashboard.py              # Streamlit 前端界面
│   └── logs/
│       └── checkpoints/              # 训练输出（自动生成）
│           ├── best_model.pt         # 最佳模型权重
│           ├── normalizer.npz        # 特征归一化参数
│           ├── training_history.json # 训练历史曲线数据
│           ├── class_accuracy.json   # 各类别准确率
│           └── model_meta.json       # 模型元信息
├── DataSet/                          # 原始 PMU 数据（只读）
│   ├── NORMAL/
│   ├── LL_FAULT/
│   ├── SLG_FAULT/
│   └── THREE_PHASE_FAULT/
├── ProcessedData/                    # 经 DataFormatter 处理后的标准 CSV（训练来源）
│   ├── NORMAL/
│   ├── LL_FAULT/
│   ├── SLG_FAULT/
│   └── THREE_PHASE_FAULT/
└── Docs/
    └── README.md                     # 本文档
```

---

## 2. 环境配置

### 2.1 Python 版本

要求 Python **3.9+**（推荐 3.9 或 3.11）。

### 2.2 安装依赖

项目使用 venv 虚拟环境，位于 `Thesis/.venv`。激活后安装依赖：

```bash
# 进入项目根目录
cd /Users/yuki/Desktop/Thesis

# 激活虚拟环境
source .venv/bin/activate

# 安装所有依赖
pip install torch numpy pandas pyyaml streamlit plotly scikit-learn
```

### 2.3 验证安装

```bash
python -c "import torch, streamlit, plotly, yaml; print('✅ All OK')"
```

> **注意**：所有 `python` 命令均需在激活虚拟环境后，或使用完整路径 `.venv/bin/python` 执行。

---

## 3. 数据说明

### 3.1 协议格式（Protocol v1.2）

训练和推理均使用统一的 16 列 CSV 格式：

| 列名 | 含义 | 单位 |
|---|---|---|
| `DFDT` | 频率变化率 | Hz/s |
| `FREQ` | 系统频率 | Hz |
| `IA_MAG` | A相电流幅值 | A |
| `IA_ANG` | A相电流角度 | deg |
| `IB_MAG` | B相电流幅值 | A |
| `IB_ANG` | B相电流角度 | deg |
| `IC_MAG` | C相电流幅值 | A |
| `IC_ANG` | C相电流角度 | deg |
| `VA_MAG` | A相电压幅值 | V |
| `VA_ANG` | A相电压角度 | deg |
| `VB_MAG` | B相电压幅值 | V |
| `VB_ANG` | B相电压角度 | deg |
| `VC_MAG` | C相电压幅值 | V |
| `VC_ANG` | C相电压角度 | deg |
| `TIMESTAMP` | 时间戳 `MM:SS.s` | — |
| `ERROR_CODE` | 故障编码（见下表） | int |

### 3.2 ERROR_CODE 编码规则

```text
ERROR_CODE = S × 100 + E
S（百位）= 严重等级：0=正常, 1=警告, 2=严重, 3=不可用
E（个位+十位）= 具体错误ID
```

| ERROR_CODE | 名称 | 含义 |
|---|---|---|
| `000` | NORMAL | 正常运行 |
| `201` | SLG_FAULT | 单相接地故障 |
| `202` | LL_FAULT | 线间故障 |
| `204` | THREE_PHASE_FAULT | 三相短路 |
| `150` | DATA_MISSING | 缺失值（NaN 行） |
| `359` | UNUSABLE_SAMPLE | 不可用样本，训练/推理时自动丢弃 |

### 3.3 数据规模

| 类别 | 原始行数 | 滑窗数（W=64, S=32） |
|---|---|---|
| NORMAL | ~3,600,000 | ~112,499 |
| SLG_FAULT | ~72,000 | ~2,249 |
| LL_FAULT | ~64,800 | ~2,024 |
| THREE_PHASE_FAULT | ~21,600 | ~674 |

> ⚠️ **类别不平衡**：NORMAL 数据量远大于故障类别。系统已自动计算逆频率权重补偿，无需手动处理。

### 3.4 缺失值处理

- 原始数据中缺失字段用 `NaN` 表示（符合 IEEE 浮点标准）
- 训练阶段：按列均值填充（`impute_nan` 函数）
- 推理阶段：同样按训练集列均值进行填充，不影响结果完整性
- `ERROR_CODE >= 300`（UNUSABLE）的行在训练和推理中均自动丢弃

---

## 4. 特征工程原理

### 4.1 极坐标 → 直角坐标转换

PMU 输出的相量数据为极坐标形式（幅值 + 角度）。根据 Protocol v1.2 Section 8 建议，将其转换为直角坐标以提升模型对相位关系的学习能力：

```text
Re = MAG × cos(ANG_rad)
Im = MAG × sin(ANG_rad)
```

对 6 个相量（IA, IB, IC, VA, VB, VC）各生成 Re 和 Im，共 12 个新特征。

### 4.2 特征向量构成

| 特征组 | 维度 | 说明 |
|---|---|---|
| DFDT | 1 | 频率变化率 |
| FREQ | 1 | 系统频率 |
| IA_Re, IA_Im | 2 | A相电流直角分量 |
| IB_Re, IB_Im | 2 | B相电流直角分量 |
| IC_Re, IC_Im | 2 | C相电流直角分量 |
| VA_Re, VA_Im | 2 | A相电压直角分量 |
| VB_Re, VB_Im | 2 | B相电压直角分量 |
| VC_Re, VC_Im | 2 | C相电压直角分量 |
| **合计** | **14** | |

### 4.3 Z-score 归一化

对每个特征列独立做 Z-score 标准化：

```text
x_norm = (x - mean) / std
```

- `mean` 和 `std` 在**训练集**上计算拟合，保存为 `logs/checkpoints/normalizer.npz`
- 推理时加载同一组参数，确保训练/推理数据分布一致
- 标准差为 0 的列（常数列）自动设为 `std = 1.0`，避免除零

### 4.4 滑动时间窗口

将时序数据切分为固定长度的重叠片段，每个窗口作为一个样本：

```text
窗口大小 (window_size) = 64 行  ≈ 0.64 秒（100Hz 采样）
滑动步长 (step_size)  = 32 行  ≈ 50% 重叠
```

窗口标签 = 窗口内所有行标签的**众数**（多数投票）。

---

## 5. 模型架构原理

系统提供三种可选架构，输入均为 `[Batch, Time=64, Features=14]`，输出为 `[Batch, 4]` 的 logits。

### 5.1 TCN（时序卷积网络）— 默认推荐

```text
输入 [B, T, 14]
  ↓  转置为 [B, 14, T]
  ↓  TCN Block 1：因果卷积，dilation=1，64 通道
  ↓  TCN Block 2：因果卷积，dilation=2，128 通道
  ↓  TCN Block 3：因果卷积，dilation=4，128 通道
  ↓  TCN Block 4：因果卷积，dilation=8，64 通道
  ↓  全局平均池化 → [B, 64]
  ↓  全连接层 → [B, 4]
```

**因果卷积**：只使用历史信息（左填充），避免未来信息泄露。  
**扩张因子**：每层 dilation 翻倍，使感受野指数增长，第 4 层可覆盖 `8×(3-1)=16` 步历史。  
**残差连接**：每个 TCNBlock 含跳跃连接，缓解梯度消失。

### 5.2 BiLSTM（双向长短时记忆网络）

```text
输入 [B, T, 14]
  ↓  BiLSTM（hidden=128, layers=2）
  ↓  取最后时刻输出 [B, 256]（正向+反向各128）
  ↓  全连接层 → [B, 4]
```

双向结构同时利用过去和未来上下文，适合对整段序列做分类。

### 5.3 Transformer Encoder

```text
输入 [B, T, 14]
  ↓  线性投影 → [B, T, d_model=64]
  ↓  正弦位置编码
  ↓  3 层 Transformer Encoder（4 头注意力，FFN=256）
  ↓  全局平均池化 → [B, 64]
  ↓  全连接层 → [B, 4]
```

自注意力机制自动学习序列内任意两时刻的依赖关系，无需手动设计感受野。

### 5.4 模型对比

| 架构 | 优势 | 适用场景 |
|---|---|---|
| **TCN** | 训练快、因果、可并行 | 首选，工程部署友好 |
| **BiLSTM** | 对长程依赖建模能力强 | 数据量充足时 |
| **Transformer** | 全局注意力、表达能力最强 | 数据量最大时 |

---

## 6. 训练流程

### 6.1 快速开始

```bash
cd /Users/yuki/Desktop/Thesis/Code

# 使用默认配置（TCN，50轮）
python train.py

# 指定模型类型
python train.py --model LSTM
python train.py --model Transformer

# 使用自定义配置文件
python train.py --config configs/train_config.yaml --model TCN
```

### 6.2 训练流程详解

```
1. 读取配置文件 (configs/train_config.yaml)
2. 扫描 ProcessedData/ 下所有类别文件夹
3. 读取每个 CSV → 过滤 UNUSABLE 行 → 提取特征矩阵
4. 对全量数据拟合 Z-score 归一化参数并保存
5. 对归一化后的特征做滑动窗口切片
6. 随机打乱后按 val_split 划分训练集/验证集
7. 计算逆频率类别权重，构建带权 CrossEntropyLoss
8. 使用 AdamW + CosineAnnealingLR 优化
9. 每轮计算训练/验证 Loss 和 Accuracy
10. 验证准确率提升时保存最优模型 (best_model.pt)
11. 达到 early_stopping_patience 轮未提升则提前停止
12. 保存训练历史、各类别准确率、模型元信息
```

### 6.3 训练输出示例

```
Epoch   Trn Loss   Trn Acc   Val Loss   Val Acc          LR
--------------------------------------------------------------
    1     0.8432    68.21%     0.7891    71.34%    1.00e-03
    2     0.6123    76.88%     0.5902    79.12%    9.99e-04
   ...
   23     0.1203    95.67%     0.1341    94.89%    5.21e-04  ★ best
```

`★ best` 表示当前轮为最佳验证准确率，已保存权重。

### 6.4 输出文件

训练完成后，`Code/logs/checkpoints/` 目录下会生成：

| 文件 | 用途 |
|---|---|
| `best_model.pt` | 最优模型权重（包含 epoch、val_acc、cfg） |
| `normalizer.npz` | 特征归一化均值和标准差（推理必需） |
| `training_history.json` | 每轮 train/val loss 和 accuracy |
| `class_accuracy.json` | 最终各类别验证准确率 |
| `model_meta.json` | 模型类型、输入维度、类别数 |

---

## 7. 推理流程

### 7.1 推理输入要求

推理 CSV 文件必须满足以下条件：

- 包含协议规定的 **16 列**，列名和顺序须与协议一致
- 数值列缺失值用 `NaN` 表示（不能用 0 或空字符串替代）
- `TIMESTAMP` 列必须存在（不能为 NaN）
- 行数建议 **≥ 64 行**（等于一个窗口大小）；行数不足时系统自动边缘填充

**合法示例（正常状态）：**
```csv
DFDT,FREQ,IA_MAG,IA_ANG,IB_MAG,IB_ANG,IC_MAG,IC_ANG,VA_MAG,VA_ANG,VB_MAG,VB_ANG,VC_MAG,VC_ANG,TIMESTAMP,ERROR_CODE
1.03e-38,49.97,7.87,121.71,8.59,-121.80,8.42,5.01,237.74,34.56,238.98,154.36,239.07,-85.28,00:00.0,0
...
```

### 7.2 推理流程详解

```
1. 加载 best_model.pt 和 normalizer.npz
2. 读取输入 CSV
3. 过滤 ERROR_CODE >= 300 的不可用行
4. 提取 14 维特征矩阵（极坐标转直角坐标）
5. NaN 列均值填充
6. 使用训练时保存的 normalizer 参数做 Z-score 归一化
7. 滑动窗口切片（W=64, S=32）
8. 批量送入模型，每窗口得到 softmax 概率 [4]
9. 对所有窗口的概率取均值 → 最终概率分布
10. 输出预测类别 + 各类别概率
```

### 7.3 推理输出解读

```json
{
  "file": "test_data.csv",
  "prediction": "SLG_FAULT",
  "confidence": "0.8732",
  "windows_analyzed": 47,
  "class_probabilities": {
    "NORMAL":            "0.041200",
    "SLG_FAULT":         "0.873200",
    "LL_FAULT":          "0.063100",
    "THREE_PHASE_FAULT": "0.022500"
  }
}
```

- **`prediction`**：置信度最高的故障类型
- **`confidence`**：该类别的平均概率（0~1）
- **`windows_analyzed`**：实际分析的滑动窗口数量
- **`class_probabilities`**：4 个类别各自的平均概率（总和为 1）

> **概率解读指导**：  
> - 主预测概率 > 0.80 → 高置信度，结果可靠  
> - 主预测概率 0.50~0.80 → 中等置信度，建议结合逐窗口分布图判断  
> - 主预测概率 < 0.50 → 低置信度，可能为混合故障或数据质量问题

---

## 8. GUI 使用说明

### 8.1 启动

```bash
cd /Users/yuki/Desktop/Thesis/Code

# 方式一：通过 app.py 启动（推荐）
python app.py

# 方式二：直接用 streamlit 命令
streamlit run GUI/dashboard.py
```

浏览器自动打开 `http://localhost:8501`

### 8.2 页面说明

#### 🏠 Home — 主页

- **数据统计卡片**：显示训练文件数量、特征维度、最佳验证准确率
- **故障类型说明**：四种故障类型及对应 ERROR_CODE
- **训练历史曲线**：使用 Plotly 可视化 Train/Val Accuracy 和 Loss 曲线（训练后刷新页面可见）

#### 🚀 Train — 训练页面

1. **展开配置面板**，调整以下参数：
   - 模型类型（TCN / LSTM / Transformer）
   - 训练轮数、批次大小、学习率
   - 学习率调度策略、早停耐心值
   - 窗口大小、滑动步长、验证集比例
2. 点击 **"💾 保存配置"** 将参数写入 `train_config.yaml`
3. 点击 **"▶️ 开始训练"** 启动后台训练进程
4. 实时训练日志滚动显示，进度条同步更新
5. 训练结束后显示最终准确率指标，页面触发庆祝动画

> ⚠️ 训练期间请勿关闭浏览器标签页，否则进度显示中断（训练进程仍在后台运行）。

#### 🔍 Inference — 推理页面

1. 点击 **"📂 选择 CSV 文件"** 上传目标文件（需符合协议格式）
2. 查看 **数据预览**（前10行）和统计摘要（总行数、NaN 比例、有效行数）
3. 系统自动完成推理，展示：
   - **主要预测标签**（彩色 Badge）及置信度百分比
   - **各类别概率条形图**（彩色动态进度条）
   - **逐窗口预测分布折线图**（Plotly 交互图，可缩放）
   - **概率汇总表格**（含均值、最大值、最小值）
4. 点击 **"⬇️ 下载推理报告"** 下载 JSON 格式结果

---

## 9. 配置文件详解

文件路径：`Code/configs/train_config.yaml`

```yaml
data:
  processed_data_root: "../ProcessedData"  # 相对于 Code/ 的数据根路径
  fault_classes:                           # 参与训练的类别（顺序对应标签 0,1,2,3）
    - NORMAL
    - SLG_FAULT
    - LL_FAULT
    - THREE_PHASE_FAULT
  drop_unusable: true      # 是否丢弃 ERROR_CODE >= 300 的行（强烈建议 true）
  drop_data_missing: false # 是否丢弃 ERROR_CODE == 150 的行（含 NaN 行）
  window_size: 64          # 滑动窗口大小（行数），影响时序感受野
  step_size: 32            # 滑动步长，越小窗口数越多、训练越慢
  val_split: 0.2           # 验证集比例
  random_seed: 42          # 随机种子，保证可复现

features:
  use_polar_to_rect: true  # 极坐标转直角坐标（建议 true）
  normalize_dfdt_freq: true

model:
  type: "TCN"              # 模型类型：TCN | LSTM | Transformer
  num_classes: 4
  input_size: 14           # 特征维度（自动更新，勿手动改）
  tcn:
    num_channels: [64, 128, 128, 64]  # 各层通道数，层数=列表长度
    kernel_size: 3
    dropout: 0.2

training:
  epochs: 50
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  scheduler: "cosine"      # cosine（推荐）| step | none
  early_stopping_patience: 10  # 验证准确率连续多少轮不提升则停止
  checkpoint_dir: "../Code/logs/checkpoints"
```

---

## 10. 注意事项与常见问题

### ⚠️ 重要注意事项

1. **不要修改 `DataFormatter.ipynb`**  
   原始数据预处理逻辑已固定，重新处理可能导致数据格式变化影响模型。

2. **推理 CSV 必须使用相同协议格式**  
   列名、列顺序、单位必须与训练数据完全一致。如果输入文件缺少 `ERROR_CODE` 列，系统会跳过过滤步骤但不会报错。

3. **归一化参数必须与训练一致**  
   推理时自动加载 `normalizer.npz`，请勿手动替换或删除此文件。如果重新训练，归一化参数会自动更新。

4. **重新训练会覆盖 `best_model.pt`**  
   如需保留旧模型，请手动备份 `logs/checkpoints/` 目录。

5. **类别不平衡已自动处理**  
   NORMAL 样本约占总数的 95%，系统通过逆频率权重自动平衡，无需手动降采样。

### ❓ 常见问题

**Q: 训练时报 `No training data found`**  
A: 检查 `ProcessedData/` 目录是否存在对应类别文件夹，以及 CSV 文件格式是否符合协议。

**Q: 推理时报 `未找到已训练模型`**  
A: 需先完成至少一次训练，生成 `logs/checkpoints/best_model.pt`。

**Q: 推理文件行数少于 64 行**  
A: 系统会自动对特征序列做边缘填充（edge padding）至窗口大小，可正常推理但结果可信度较低，建议提供更长的时序片段。

**Q: 推理概率分布过于均匀（各类约 25%）**  
A: 可能原因：① 输入数据质量差（大量 NaN）② 数据不符合协议格式 ③ 模型未充分训练（训练轮数不足）。

**Q: 在 M1/M2 Mac 上训练速度慢**  
A: PyTorch 支持 Apple Silicon MPS 加速，系统已自动检测并使用 `mps` 设备，无需手动配置。

**Q: 如何添加新的故障类型**  
A: 需要同时修改以下几处：  
① `ProcessedData/` 下添加新类别文件夹和 CSV  
② `configs/train_config.yaml` 中 `fault_classes` 列表新增类别名称  
③ `models/feature_engineering.py` 中 `CLASS_NAMES`、`CLASS_CODES`、`LABEL_MAP` 同步更新  
④ `model.num_classes` 修改为新的类别总数  
⑤ 重新训练

---

## 11. 故障类型参考

| 标签 | ERROR_CODE | 名称 | 物理描述 | 典型特征 |
|---|---|---|---|---|
| 0 | 000 | NORMAL | 系统正常运行 | 三相对称，频率稳定在 50/60Hz |
| 1 | 201 | SLG_FAULT | 单相接地故障 | 一相电流激增，对应相电压跌落，零序分量出现 |
| 2 | 202 | LL_FAULT | 线间（相间）故障 | 两相电流不对称增大，线电压下降，正负序不平衡 |
| 3 | 204 | THREE_PHASE_FAULT | 三相短路故障 | 三相电流大幅升高，三相电压骤降，DFDT 剧烈变化 |

---

*文档版本：v1.0 | 协议版本：PMU Protocol v1.2 | 生成日期：2026-02-23*
